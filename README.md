# deep learning fundamentals kaggle course
in this course today i go through some basements in ML, DL, and AI as well and it is so cool and fruitful;
1. At first building a model is a simple task that should be done in a way that yo can see after the dataset
has been loaded in the code.
2. The second step is to use activation functions in order to go beyond line and plane space;
3. The third step is to use optimizer, loss functions and training data to intercept in the process of
learning that should occur for our model.
4. learning rate show how big is the steps of our model to change it's direction towards final solution
and finding the best fit for learning rate is a problem and it should be solved by a method called optimizer.
5. The final hint of the day is that adam optimizer can be genaralized to many tasks and regression
problems usually require "mae" loss function or other similar functions.
